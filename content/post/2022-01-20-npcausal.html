---
author: Xiang Ao
categories:
- R
date: "2022-01-20"
slug: npcausal
tags:
- plot
- regression
- R
title: Recent causal inference tools
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In this post, I’ll use simulated data to see how a few recently developed causal inference packages work.</p>
<p>In other posts, I have discussed some methods involving machine learning, such as TMLE, or causal forest. This time we’ll talk about nonparametric methods and double machine learning.</p>
<div id="identification" class="section level1">
<h1>Identification</h1>
<p>First suppose we have a “target” parameter in mind, which is some function of the some unknown distribution <span class="math inline">\(P^*\)</span>, called a functional, say <span class="math inline">\(\psi^*(P^*)\)</span>. For example ATE, <span class="math inline">\(E(Y^1- Y^0)\)</span>.</p>
<p>The goal is to have <span class="math inline">\(\psi^*(P^*)\)</span> estimated by <span class="math inline">\(\psi(\hat P)\)</span>, where <span class="math inline">\(P\)</span> is some observational distribution, and <span class="math inline">\(\psi\)</span> is some known function. This is called identification.</p>
<p>For example, suppose we are intreseted in the ATE, <span class="math inline">\(E[Y^1 - Y^0]\)</span>. The identification is done by deriving from <span class="math inline">\(E(Y^a | X) = E(Y | X, A=a)\)</span>; basically from the target parameter, which is counterfactual, to something estimable from the observed variables in the sample. This identification can only be done with assumptions such as ignorability, positivity and consistency.</p>
<p>In this case, <span class="math inline">\(\psi^* = E(Y^a)\)</span>, and <span class="math inline">\(\psi = E \{ E(Y | X) \}\)</span>.</p>
</div>
<div id="nonparametric-estimation-in-a-nutshell" class="section level1">
<h1>Nonparametric estimation in a nutshell</h1>
<p>After idenfication, we then need to find a way to construct a good estimator <span class="math inline">\(\hat \psi\)</span> for <span class="math inline">\(\psi (P)\)</span>.</p>
<p>Suppose we have a sample <span class="math inline">\(Z\)</span>, which can be <span class="math inline">\({A,W,Y}\)</span> (treatment, covariates, and outcome) in causal inference situation.</p>
<p>We can use a parametric model, assuming we know what kind of distribution <span class="math inline">\(P\)</span> is. We assume <span class="math inline">\(P=P_\theta\)</span>. Then we can use MLE to get <span class="math inline">\(\psi(P_\theta)\)</span>. This is what we do in parametric modeling. But this can be too restrictive, we usually have no way of knowing how good <span class="math inline">\(P_\theta\)</span> is; in other words, the model can be misspecified thus the target parameter is estimated with bias.</p>
<p>If we don’t make any assumptions about the distribution, then we are in nonparametric world. The natural way to approximate <span class="math inline">\(P\)</span> is to use the empirical distribution <span class="math inline">\(\hat P\)</span>. Then <span class="math inline">\(\psi^*(\hat P)\)</span> is the “plug-in” estimator. For example we can use sample mean to estimate the population mean, etc. But “plug-in” estimator is not the best choice, in general, because they are not <span class="math inline">\(\sqrt n\)</span> consistent. There is usually a “plug-in bias” when using “plug-in” estimator.</p>
<p>Another choice is nonparametric estimator based on influence function. The nice thing about this type of estimators based on efficient influence function is that they are often <span class="math inline">\(\sqrt n\)</span> consistent, meaning they converge to the truth as fast as <span class="math inline">\(\sqrt n\)</span> rate. This means <span class="math inline">\(\hat \psi - \psi\)</span> not only goes to 0 with <span class="math inline">\(n\)</span> goes to infinity, but also as fast as <span class="math inline">\(1/ \sqrt n\)</span> goes to 0. Why does this matter? Because we have limited sample size, when we are based on asymptotics, the faster the bias goes to 0 the smaller sample size we need.</p>
<p>If we have an estimator such that
<span class="math display">\[ \sqrt n (\hat \psi - \psi) = \sqrt n P_n(\phi) + o_P(1) \rightarrow N(0, var(\phi))\]</span></p>
<p>Then this estimator is <span class="math inline">\(\sqrt n\)</span> consistent and asympotically normal; and <span class="math inline">\(\phi\)</span> is the influence function. The efficient influence function is the one with variance at the lower bound (similar to parametric case for Cramer-Rao lower bound).</p>
<p>For ATE, target parameter <span class="math inline">\(\psi = E \{ E(Y | X, A=1) \}\)</span>, the efficient influence function is
<span class="math display">\[\phi (Z; P) = \frac{A}{\pi (X)} \{ Y - \mu (X) \} + \mu(X) - \psi\]</span>
where <span class="math inline">\(\pi (X) = P(A=1 | X)\)</span> the propensity score model, <span class="math inline">\(\mu (X) = E(Y |X, A=1)\)</span>, the outcome model.</p>
<p>The idea of IF-based estimator is to use so called “one-step correction” to estimate the “plug-in” bias. The bias is approximated by first deriving the influence function; then based on the influence function, the bias can be estimated by using the influence function as a slope to the path from “plug-in” estimate” to true parameter. This path can be non-linear; therefore this one-step correction may not be perfect, but it should be closer to the truth than the original estimate.</p>
<p>For example, an IF-based bias-corrected estimator
<span class="math display">\[\hat \psi = P_n [ \frac{A}{\hat \pi (X)} \{ Y - \hat \mu (X) \} + \hat \mu(X) ]\]</span>
where <span class="math inline">\(P_n\)</span> means sample mean. This is the familiar AIPW estimator. It is known to be doubly robust. Since we know the influence function, we can calculate its variance. This variance is the same as variance of <span class="math inline">\(\hat \psi\)</span> since they differ by a constant <span class="math inline">\(\psi\)</span>. We can use any estimators to estimate the nuisance parameters, then get <span class="math inline">\(\hat \psi\)</span>. Then the sample variance is the variance of the influence function; and the sample mean is the ATE.</p>
<p>In this estimator, both <span class="math inline">\(\hat \mu\)</span> and <span class="math inline">\(\hat \pi\)</span> can be done using any estimator, such as machine learning.</p>
<p>The IF based estimator only works if either <span class="math inline">\(\phi(P)\)</span> is not too complex (empirical processes theory involved, a lot of difficult situations won’t apply), or we separate <span class="math inline">\(\hat P\)</span> and <span class="math inline">\(P_n\)</span> to prevent overfitting. The latter is by sample splitting. Sample splitting is usually preferred because it needs less assumption than empirical processes. The idea is to split the sample, do estimation of nuisance parameters (such as <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\pi\)</span>) in one sample, and construct estimator in the other. This way it is shown the EIF based estimator is <span class="math inline">\(\sqrt n\)</span> consistent and asympotically normal; the variance is the variance of the influence function.</p>
<p>Unfortunately influence function is hard to derive for different situations. Ed Kennedy has done a lot of work on using IF on causal inference <a href="http://www.ehkennedy.com/" class="uri">http://www.ehkennedy.com/</a>. Here we use his “npcausal” package.</p>
</div>
<div id="simulation-1" class="section level1">
<h1>simulation 1</h1>
<p>The simulated data is from <a href="https://migariane.github.io/TMLE.nb.html" class="uri">https://migariane.github.io/TMLE.nb.html</a></p>
<pre class="r"><code>library(npcausal)
library(boot)
library(MASS) 
library(SuperLearner)
library(tmle)
library(tidyverse)
library(ranger)</code></pre>
<pre class="r"><code>set.seed(123)
n=1000  
w1 &lt;- rbinom(n, size=1, prob=0.5)
w2 &lt;- rbinom(n, size=1, prob=0.65)
w3 &lt;- round(runif(n, min=0, max=4), digits=3)
w4 &lt;- round(runif(n, min=0, max=5), digits=3)
A  &lt;- rbinom(n, size=1, prob= plogis(-0.4 + 0.2*w2 + 0.15*w3 + 0.2*w4 + 0.15*w2*w4))
Y.1 &lt;- rbinom(n, size=1, prob= plogis(-1 + 1 -0.1*w1 + 0.3*w2 + 0.25*w3 + 0.2*w4 + 0.15*w2*w4))
Y.0 &lt;- rbinom(n, size=1, prob= plogis(-1 + 0 -0.1*w1 + 0.3*w2 + 0.25*w3 + 0.2*w4 + 0.15*w2*w4))
Y &lt;- Y.1*A + Y.0*(1 - A)
W&lt;-data.frame(cbind(w1,w2,w3,w4))
data &lt;-  data.frame(w1, w2, w3, w4, A, Y, Y.1, Y.0)
True_Psi &lt;- mean(data$Y.1-data$Y.0);
cat(&quot; True_Psi:&quot;, True_Psi)</code></pre>
<pre><code>##  True_Psi: 0.203</code></pre>
<p>We first include a few learners. “npcausal” has a set of default learners.</p>
<pre class="r"><code>SL.library &lt;- c(&quot;SL.earth&quot;,&quot;SL.gam&quot;,&quot;SL.glm&quot;,&quot;SL.glmnet&quot;,&quot;SL.glm.interaction&quot;, &quot;SL.mean&quot;,&quot;SL.ranger&quot;, &quot;SL.xgboost&quot;)</code></pre>
<div id="plug-in-estimator-from-outcome-regression" class="section level2">
<h2>plug-in estimator from outcome regression</h2>
<pre class="r"><code>SL.outcome&lt;- SuperLearner(Y=data$Y, X=data %&gt;% select(w1,w2,w3,w4,A),
                                     SL.library=SL.library, family=&quot;binomial&quot;)
SL.outcome.obs&lt;- predict(SL.outcome, X=data %&gt;% select(w1,w2,w3,w4,A))$pred
# predict the PO Y^1
SL.outcome.1&lt;- predict(SL.outcome, newdata=data %&gt;% select(w1,w2,w3,w4,A) %&gt;% mutate(A=1))$pred
# predict the PO Y^0
SL.outcome.0&lt;- predict(SL.outcome, newdata=data %&gt;% select(w1,w2,w3,w4,A) %&gt;% mutate(A=0))$pred</code></pre>
<p>This is to do machine learning on the outcome equation; then get prediction on treated vs control, to get <span class="math inline">\(\hat Y^1\)</span> and <span class="math inline">\(\hat Y^0\)</span>. The sample average of the difference is our plug-in estimator.</p>
<pre class="r"><code>SL.plugin&lt;-mean(SL.outcome.1-SL.outcome.0)
SL.plugin</code></pre>
<pre><code>## [1] 0.2072614</code></pre>
<p>It’s not too far from the truth in this case.</p>
<pre class="r"><code>Q=data.frame(QAW=SL.outcome.obs, Q0W=SL.outcome.0,Q1W=SL.outcome.1)</code></pre>
</div>
<div id="aipw-with-machine-learning" class="section level2">
<h2>AIPW with machine learning</h2>
<p>We can use “npcausal” to do AIPW, or we can manually calculate it.</p>
<p>First we get propensity score. Then, based on both <span class="math inline">\(\hat \mu\)</span> and <span class="math inline">\(\hat \pi\)</span>, we can get the AIPW estimator.</p>
<pre class="r"><code>set.seed(123)
SL.g&lt;- SuperLearner(Y=data$A, X=data %&gt;% select(w1,w2,w3,w4),
                    SL.library=SL.library, family=&quot;binomial&quot;)
g1W &lt;- SL.g$SL.predict
g0W&lt;- 1- g1W
IF.1&lt;-((data$A/g1W)*(data$Y-Q$Q1W)+Q$Q1W)
IF.0&lt;-(((1-data$A)/g0W)*(data$Y-Q$Q0W)+Q$Q0W)
IF&lt;-IF.1-IF.0
aipw.1&lt;-mean(IF.1);aipw.0&lt;-mean(IF.0)
aipw.manual&lt;-aipw.1-aipw.0
ci.lb&lt;-mean(IF)-qnorm(.975)*sd(IF)/sqrt(length(IF))
ci.ub&lt;-mean(IF)+qnorm(.975)*sd(IF)/sqrt(length(IF))
res.manual.aipw&lt;-c(aipw.manual,ci.lb, ci.ub)
res.manual.aipw</code></pre>
<pre><code>## [1] 0.2121542 0.1514396 0.2728689</code></pre>
<p>Now we use “npcausal”.</p>
<pre class="r"><code>library(npcausal)
aipw&lt;- ate(y=Y, a=A, x=W, nsplits=1, sl.lib=SL.library)</code></pre>
<pre><code>## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |====================================================                  |  75%
  |                                                                            
  |======================================================================| 100%
##      parameter       est         se     ci.ll     ci.ul pval
## 1      E{Y(0)} 0.5740347 0.02704671 0.5210231 0.6270462    0
## 2      E{Y(1)} 0.7860537 0.01599838 0.7546969 0.8174106    0
## 3 E{Y(1)-Y(0)} 0.2120191 0.03109855 0.1510659 0.2729722    0</code></pre>
<pre class="r"><code>aipw$res</code></pre>
<pre><code>##      parameter       est         se     ci.ll     ci.ul pval
## 1      E{Y(0)} 0.5740347 0.02704671 0.5210231 0.6270462    0
## 2      E{Y(1)} 0.7860537 0.01599838 0.7546969 0.8174106    0
## 3 E{Y(1)-Y(0)} 0.2120191 0.03109855 0.1510659 0.2729722    0</code></pre>
<p>The results from “npcausal” are almost the same as the manual results. Note here we did not do sample splitting; therefore this is not “bias-corrected” estimator. Sample splitting is to help prevent overfitting, thus reducing bias. “npcausal” does this for you, if you specify “nsplits” option.</p>
<pre class="r"><code>aipw2&lt;- ate(y=Y, a=A, x=W, nsplits=10)</code></pre>
<pre><code>## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |==                                                                    |   2%
  |                                                                            
  |====                                                                  |   5%
  |                                                                            
  |=====                                                                 |   8%
  |                                                                            
  |=======                                                               |  10%
  |                                                                            
  |=========                                                             |  12%
  |                                                                            
  |==========                                                            |  15%
  |                                                                            
  |============                                                          |  18%
  |                                                                            
  |==============                                                        |  20%
  |                                                                            
  |================                                                      |  22%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |===================                                                   |  28%
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |=======================                                               |  32%
  |                                                                            
  |========================                                              |  35%
  |                                                                            
  |==========================                                            |  38%
  |                                                                            
  |============================                                          |  40%
  |                                                                            
  |==============================                                        |  42%
  |                                                                            
  |================================                                      |  45%
  |                                                                            
  |=================================                                     |  48%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |=====================================                                 |  52%
  |                                                                            
  |======================================                                |  55%
  |                                                                            
  |========================================                              |  58%
  |                                                                            
  |==========================================                            |  60%
  |                                                                            
  |============================================                          |  62%
  |                                                                            
  |==============================================                        |  65%
  |                                                                            
  |===============================================                       |  68%
  |                                                                            
  |=================================================                     |  70%
  |                                                                            
  |===================================================                   |  72%
  |                                                                            
  |====================================================                  |  75%
  |                                                                            
  |======================================================                |  78%
  |                                                                            
  |========================================================              |  80%
  |                                                                            
  |==========================================================            |  82%
  |                                                                            
  |============================================================          |  85%
  |                                                                            
  |=============================================================         |  88%
  |                                                                            
  |===============================================================       |  90%
  |                                                                            
  |=================================================================     |  92%
  |                                                                            
  |==================================================================    |  95%
  |                                                                            
  |====================================================================  |  98%
  |                                                                            
  |======================================================================| 100%
##      parameter       est         se     ci.ll     ci.ul pval
## 1      E{Y(0)} 0.5720028 0.02882336 0.5155090 0.6284966    0
## 2      E{Y(1)} 0.7846837 0.01628671 0.7527618 0.8166057    0
## 3 E{Y(1)-Y(0)} 0.2126809 0.03276098 0.1484694 0.2768925    0</code></pre>
</div>
<div id="tmle" class="section level2">
<h2>TMLE</h2>
<p>We can use TMLE for ATE.</p>
<pre class="r"><code>library(tmle)
TMLE&lt;- tmle(Y=data$Y,A=data$A,W=W, family=&quot;binomial&quot;, Q.SL.library=SL.library, g.SL.library=SL.library)

TMLE$estimates$ATE</code></pre>
<pre><code>## $psi
## [1] 0.2125673
## 
## $var.psi
## [1] 0.001036017
## 
## $CI
## [1] 0.1494804 0.2756543
## 
## $pvalue
## [1] 3.999638e-11</code></pre>
</div>
<div id="doubleml" class="section level2">
<h2>DoubleML</h2>
<p>The R package DoubleML implements the double/debiased machine learning framework of Chernozhukov et al. (2018). It provides functionalities to estimate parameters in causal models based on machine learning methods. The double machine learning framework consist of three key ingredients: Neyman orthogonality, High-quality machine learning estimation and Sample splitting.</p>
<p>They consider a partially linear model:</p>
<p><span class="math display">\[ y_i = \theta d_i + g_0(x_i) + \eta_i \]</span></p>
<p><span class="math display">\[ d_i = m_0(x_i) + v_i \]</span></p>
<p>This model is quite general, except it does not allow interaction of <span class="math inline">\(d\)</span> and <span class="math inline">\(x\)</span>; therefore no hetergeneous treatment effect across <span class="math inline">\(x\)</span>. But “DoubleML” implements more than partially linear model, it actually allows for heterogeneous treatment effects, in models such as interactive regression model.</p>
<p>The basic idea of doubleML is to use any machine learning algorithm to estimate outcome equation (<span class="math inline">\(l_0(X) = E(Y | X)\)</span>) and treatment equation (<span class="math inline">\(m_0(X) = E(D | X)\)</span>). Then get the residuals, namely <span class="math inline">\(\hat W=Y-\hat l_0(X)\)</span> and <span class="math inline">\(\hat V = D - \hat m_0(X)\)</span>.</p>
<p>Then regress <span class="math inline">\(\hat W\)</span> on <span class="math inline">\(\hat V\)</span>. Based on FWL theorem, you get <span class="math inline">\(\hat \theta\)</span>.</p>
<p>An important component here is to specify a Neyman-orthogonal score function. In the case of PLR, it can be the product of the two residuals:</p>
<p><span class="math display">\[\psi (W; \theta, \eta) = (Y-D\theta -g(X))(D-m(X)) \]</span></p>
<p>The estimators <span class="math inline">\(\hat \theta\)</span> is to solve the equation that the sample mean of this score function being 0.</p>
<p>And the variance of this score function is used as the variance of the doubleML estimator’s variance.</p>
<pre class="r"><code>library(DoubleML)
dml_data = DoubleMLData$new(data,
                             y_col = &quot;Y&quot;,
                             d_cols = &quot;A&quot;,
                             x_cols = c(&quot;w1&quot;,&quot;w2&quot;,&quot;w3&quot;,&quot;w4&quot;))
print(dml_data)</code></pre>
<pre><code>## ================= DoubleMLData Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: Y
## Treatment variable(s): A
## Covariates: w1, w2, w3, w4
## Instrument(s): 
## No. Observations: 1000</code></pre>
<pre class="r"><code>library(mlr3)
library(mlr3learners)
# surpress messages from mlr3 package during fitting
lgr::get_logger(&quot;mlr3&quot;)$set_threshold(&quot;warn&quot;)

learner = lrn(&quot;regr.ranger&quot;, num.trees=500,  max.depth=5, min.node.size=2)
ml_g = learner$clone()
ml_m = learner$clone()

learner = lrn(&quot;regr.glmnet&quot;)
ml_g_sim = learner$clone()
ml_m_sim = learner$clone()

set.seed(3141)
obj_dml_plr = DoubleMLPLR$new(dml_data, ml_g=ml_g, ml_m=ml_m)
obj_dml_plr$fit()
print(obj_dml_plr)</code></pre>
<pre><code>## ================= DoubleMLPLR Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: Y
## Treatment variable(s): A
## Covariates: w1, w2, w3, w4
## Instrument(s): 
## No. Observations: 1000
## 
## ------------------ Score &amp; algorithm ------------------
## Score function: partialling out
## DML algorithm: dml2
## 
## ------------------ Machine learner   ------------------
## ml_g: regr.ranger
## ml_m: regr.ranger
## 
## ------------------ Resampling        ------------------
## No. folds: 5
## No. repeated sample splits: 1
## Apply cross-fitting: TRUE
## 
## ------------------ Fit summary       ------------------
##  Estimates and significance testing of the effect of target variables
##   Estimate. Std. Error t value Pr(&gt;|t|)    
## A   0.23271    0.03224   7.219 5.25e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
<div id="simulation-2" class="section level1">
<h1>simulation 2</h1>
<p>This simulation is based on a package “AIPW”, which has a simulation data “eager_sim_rct”. <span class="math inline">\(A\)</span> denotes the binary treatment assignment, <span class="math inline">\(Y\)</span> is the binary outcome, and <span class="math inline">\(W_g\)</span> represents the covariate that affects the treatment assignment, which in our case was deemed to be the eligibility stratum indicator, sampled with replacement from the EAGeR Trial. Similarly, <span class="math inline">\(W_Q\)</span> is a set of baseline prognostic covariates, which were also sampled with replacement from the EAGeR Trial, and includes the number of prior pregnancy losses, age, number of months of trying to conceive prior to randomization, body mass index (weight (kg)/height (m)2), and mean arterial blood pressure (denoted W1…5 , respectively).</p>
<p>Data and the true risk difference:</p>
<pre class="r"><code>library(AIPW)
data(&quot;eager_sim_rct&quot;)
glimpse(eager_sim_rct)</code></pre>
<pre><code>## Rows: 1,228
## Columns: 8
## $ sim_Y             &lt;int&gt; 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0…
## $ sim_Tx            &lt;int&gt; 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0…
## $ eligibility       &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0…
## $ loss_num          &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1…
## $ age               &lt;dbl&gt; 21.93, 30.65, 30.96, 32.24, 31.49, 27.99, 31.60, 25.…
## $ time_try_pregnant &lt;dbl&gt; 2, 1, 0, 9, 1, 10, 8, 6, 1, 0, 1, 1, 3, 3, 0, 10, 10…
## $ BMI               &lt;dbl&gt; 19.51963, 25.56474, 20.75446, 21.65728, 24.27480, 19…
## $ meanAP            &lt;dbl&gt; 73.33333, 79.00000, 79.33333, 63.83333, 97.44444, 85…</code></pre>
<pre class="r"><code>attributes(eager_sim_rct)$true_rd</code></pre>
<pre><code>## [1] 0.1324569</code></pre>
<pre class="r"><code>data &lt;- eager_sim_rct %&gt;%
  rename(Y=sim_Y, A=sim_Tx) %&gt;%
  tibble()

W &lt;- data %&gt;%
  select(-Y,-A)</code></pre>
<div id="plug-in-estimator-from-outcome-regression-1" class="section level2">
<h2>plug-in estimator from outcome regression</h2>
<pre class="r"><code>SL.outcome&lt;- SuperLearner(Y=data$Y, X=data %&gt;% select(-Y),
                                     SL.library=SL.library, family=&quot;binomial&quot;)
SL.outcome.obs&lt;- predict(SL.outcome, X=data %&gt;% select(-Y))$pred
# predict the PO Y^1
SL.outcome.1&lt;- predict(SL.outcome, newdata=data %&gt;% select(-Y) %&gt;% mutate(A=1))$pred
# predict the PO Y^0
SL.outcome.0&lt;- predict(SL.outcome, newdata=data %&gt;% select(-Y) %&gt;% mutate(A=0))$pred</code></pre>
<p>This is to do machine learning on the outcome equation; then get prediction on treated vs control, to get <span class="math inline">\(\hat Y^1\)</span> and <span class="math inline">\(\hat Y^0\)</span>. The sample average of the difference is our plug-in estimator.</p>
<pre class="r"><code>SL.plugin&lt;-mean(SL.outcome.1-SL.outcome.0)
SL.plugin</code></pre>
<pre><code>## [1] 0.1206068</code></pre>
<pre class="r"><code>Q=data.frame(QAW=SL.outcome.obs, Q0W=SL.outcome.0,Q1W=SL.outcome.1)</code></pre>
</div>
<div id="aipw-with-machine-learning-1" class="section level2">
<h2>AIPW with machine learning</h2>
<pre class="r"><code>set.seed(123)
SL.g&lt;- SuperLearner(Y=data$A, X=data %&gt;% select(-Y, -A),
                    SL.library=SL.library, family=&quot;binomial&quot;)
g1W &lt;- SL.g$SL.predict
g0W&lt;- 1- g1W
IF.1&lt;-((data$A/g1W)*(data$Y-Q$Q1W)+Q$Q1W)
IF.0&lt;-(((1-data$A)/g0W)*(data$Y-Q$Q0W)+Q$Q0W)
IF&lt;-IF.1-IF.0
aipw.1&lt;-mean(IF.1);aipw.0&lt;-mean(IF.0)
aipw.manual&lt;-aipw.1-aipw.0
ci.lb&lt;-mean(IF)-qnorm(.975)*sd(IF)/sqrt(length(IF))
ci.ub&lt;-mean(IF)+qnorm(.975)*sd(IF)/sqrt(length(IF))
res.manual.aipw&lt;-c(aipw.manual,ci.lb, ci.ub)
res.manual.aipw</code></pre>
<pre><code>## [1] 0.13093060 0.06941035 0.19245086</code></pre>
<p>Now we use “npcausal”.</p>
<pre class="r"><code>library(npcausal)
aipw&lt;- ate(y=data$Y, a=data$A, x=W, nsplits=1, sl.lib=SL.library)</code></pre>
<pre><code>## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |====================================================                  |  75%
  |                                                                            
  |======================================================================| 100%
##      parameter       est         se      ci.ll     ci.ul pval
## 1      E{Y(0)} 0.4463680 0.02281026 0.40165986 0.4910761    0
## 2      E{Y(1)} 0.5738675 0.02101274 0.53268257 0.6150525    0
## 3 E{Y(1)-Y(0)} 0.1274996 0.03086141 0.06701122 0.1879879    0</code></pre>
<p>The results from “npcausal” are almost the same as the manual results. Note here we did not do sample splitting; therefore this is not “bias-corrected” estimator. Sample splitting is to help prevent overfitting, thus reducing bias. “npcausal” does this for you, if you specify “nsplits” option.</p>
<pre class="r"><code>aipw2&lt;- ate(y=data$Y, a=data$A, x=W, nsplits=10)</code></pre>
<pre><code>## 
  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |==                                                                    |   2%
  |                                                                            
  |====                                                                  |   5%
  |                                                                            
  |=====                                                                 |   8%
  |                                                                            
  |=======                                                               |  10%
  |                                                                            
  |=========                                                             |  12%
  |                                                                            
  |==========                                                            |  15%
  |                                                                            
  |============                                                          |  18%
  |                                                                            
  |==============                                                        |  20%
  |                                                                            
  |================                                                      |  22%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |===================                                                   |  28%
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |=======================                                               |  32%
  |                                                                            
  |========================                                              |  35%
  |                                                                            
  |==========================                                            |  38%
  |                                                                            
  |============================                                          |  40%
  |                                                                            
  |==============================                                        |  42%
  |                                                                            
  |================================                                      |  45%
  |                                                                            
  |=================================                                     |  48%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |=====================================                                 |  52%
  |                                                                            
  |======================================                                |  55%
  |                                                                            
  |========================================                              |  58%
  |                                                                            
  |==========================================                            |  60%
  |                                                                            
  |============================================                          |  62%
  |                                                                            
  |==============================================                        |  65%
  |                                                                            
  |===============================================                       |  68%
  |                                                                            
  |=================================================                     |  70%
  |                                                                            
  |===================================================                   |  72%
  |                                                                            
  |====================================================                  |  75%
  |                                                                            
  |======================================================                |  78%
  |                                                                            
  |========================================================              |  80%
  |                                                                            
  |==========================================================            |  82%
  |                                                                            
  |============================================================          |  85%
  |                                                                            
  |=============================================================         |  88%
  |                                                                            
  |===============================================================       |  90%
  |                                                                            
  |=================================================================     |  92%
  |                                                                            
  |==================================================================    |  95%
  |                                                                            
  |====================================================================  |  98%
  |                                                                            
  |======================================================================| 100%
##      parameter       est         se      ci.ll     ci.ul pval
## 1      E{Y(0)} 0.4437742 0.02523783 0.39430808 0.4932404    0
## 2      E{Y(1)} 0.5755262 0.02312439 0.53020242 0.6208500    0
## 3 E{Y(1)-Y(0)} 0.1317520 0.03411870 0.06487934 0.1986247    0</code></pre>
</div>
<div id="doubleml-1" class="section level2">
<h2>DoubleML</h2>
<pre class="r"><code>library(DoubleML)
dml_data = DoubleMLData$new(as.data.frame(data),
                             y_col = &quot;Y&quot;,
                             d_cols = &quot;A&quot;,
                             x_cols = c(&quot;eligibility&quot; , &quot;loss_num&quot;,&quot;age&quot;, &quot;time_try_pregnant&quot;,&quot;BMI&quot;,&quot;meanAP&quot;))
print(dml_data)</code></pre>
<pre><code>## ================= DoubleMLData Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: Y
## Treatment variable(s): A
## Covariates: eligibility, loss_num, age, time_try_pregnant, BMI, meanAP
## Instrument(s): 
## No. Observations: 1228</code></pre>
<pre class="r"><code>library(mlr3)
library(mlr3learners)
# surpress messages from mlr3 package during fitting
lgr::get_logger(&quot;mlr3&quot;)$set_threshold(&quot;warn&quot;)

learner = lrn(&quot;regr.ranger&quot;, num.trees=500,  max.depth=5, min.node.size=2)
ml_g = learner$clone()
ml_m = learner$clone()

learner = lrn(&quot;regr.glmnet&quot;)
ml_g_sim = learner$clone()
ml_m_sim = learner$clone()

set.seed(3141)
obj_dml_plr = DoubleMLPLR$new(dml_data, ml_g=ml_g, ml_m=ml_m)
obj_dml_plr$fit()
print(obj_dml_plr)</code></pre>
<pre><code>## ================= DoubleMLPLR Object ==================
## 
## 
## ------------------ Data summary      ------------------
## Outcome variable: Y
## Treatment variable(s): A
## Covariates: eligibility, loss_num, age, time_try_pregnant, BMI, meanAP
## Instrument(s): 
## No. Observations: 1228
## 
## ------------------ Score &amp; algorithm ------------------
## Score function: partialling out
## DML algorithm: dml2
## 
## ------------------ Machine learner   ------------------
## ml_g: regr.ranger
## ml_m: regr.ranger
## 
## ------------------ Resampling        ------------------
## No. folds: 5
## No. repeated sample splits: 1
## Apply cross-fitting: TRUE
## 
## ------------------ Fit summary       ------------------
##  Estimates and significance testing of the effect of target variables
##   Estimate. Std. Error t value Pr(&gt;|t|)    
## A   0.12505    0.03292   3.798 0.000146 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
</div>
