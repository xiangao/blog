<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.92.1" />


<title>红楼梦 作者分析 - A Hugo website</title>
<meta property="og:title" content="红楼梦 作者分析 - A Hugo website">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About me</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">红楼梦 作者分析</h1>

    
    <span class="article-date">2017/09/19</span>
    

    <div class="article-content">
      


<div class="section level1">
<h1>简介</h1>
<p>在本文中我们用机器学习和统计的方法来分析红楼梦的作者。红楼梦是中国最著名的小说之一(<a href="https://en.wikipedia.org/wiki/Dream_of_the_Red_Chamber" class="uri">https://en.wikipedia.org/wiki/Dream_of_the_Red_Chamber</a>)。 大多数人认为前八十回为曹雪芹所作， 后四十回为后人续作。 除非我们从考古中发现证据，唯一能告诉我们的便是小说本身。我们可以从文章的用词风格来判断作者。 即使续作者尽量模仿原作者的风格，也很难不在字里行间露出自己固有的风格。（我们也许可以用同样的方法来鉴定真画和假画，但是需要有原作者的大量画作）。</p>
<p>我是从这里下载的红楼梦原著120回版（具体哪个版本我也没仔细研究）: <a href="http://www.shuyaya.cc/book/2034/#download" class="uri">http://www.shuyaya.cc/book/2034/#download</a></p>
<p>我用了R中的 “Rwordseg” 来做中文分词。 就是把一句话分成词和词组。 然后用了 “cleanNLP” 得到分词频率矩阵 （“term frenquency matrix”）。 然后在最后一个模型用到 “topicmodels”。</p>
</div>
<div class="section level1">
<h1>读入文本进行分词</h1>
<p>这一部分就是读入文本， 把它分为120回，每一回作为一个文本， 然后分词。</p>
<pre class="r"><code># analysis starts here
library(rticles)
library(cleanNLP)
library(readr)
library(stringi)
library(ggplot2)
library(glmnet)
library(ggrepel)
library(viridis)
library(magrittr)

library(topicmodels)
library(tidyverse)
library(rJava)
library(Rwordseg)
library(RColorBrewer)
library(tm)

require(readtext)
honglou1 &lt;- readtext(&quot;~/projects/honglongmeng/honglou1.txt&quot;, text_field = &quot;texts&quot;)

# here is to split into chapters using stringr&#39;s splitting functions
my_split &lt;- function(text) {
    pattern &lt;- &#39;第.{1,3}回 &#39;
    x &lt;- str_split(text, pattern)[[1]]
    y &lt;- str_extract_all(text, pattern)[[1]]
    data.frame(
        chapter = (1:length(x)) - 1,
        text = str_trim(x),
        header = c(y, &quot;&quot;)
    )
}

chaps &lt;- my_split(honglou1$text)


hong &lt;- chaps %&gt;%
    mutate(txt = as.character(text))

# use Rwordseg&#39;s segmentCN to get tokens from text.
honglou &lt;- tbl_df(hong) %&gt;%
     mutate(token=segmentCN(txt)) %&gt;%
     mutate(id=chapter) %&gt;%
     select(id, token)</code></pre>
</div>
<div class="section level1">
<h1>第一种模型</h1>
<p>生成分词以后， 我用“cleanNLP”里的一个函数叫“get_tfidf”来取得分词频率矩阵。</p>
<p>首先我想到的办法是把所有章节分为“training” 和“test”。 这是机器学习的常用办法。 在“training” 章节中， 如果是前八十章， 我标识作者为“Cao”，否则标识为“unknown”。</p>
<p>然后我用“elastic net” (combination of lasso and ridge) （一种机器学习方法）在training sample。 然后在test sample中来预测作者。</p>
<p>这样我们得到每一章作者是曹雪芹的概率。</p>
<pre class="r"><code># make it long format: from each row for each chapter to one row for each token.
hongloumeng &lt;- honglou %&gt;%
    group_by(id) %&gt;%
    unnest(token) %&gt;%
    filter(id&gt;0) %&gt;%
    ungroup()

# get length (how many tokens in each chapter) on the way for each chapter
hongloumeng %&gt;%
  group_by(id) %&gt;%
  summarize(sent_len = n()) %$%
  quantile(sent_len, seq(0,1,0.1))</code></pre>
<pre><code>##     0%    10%    20%    30%    40%    50%    60%    70%    80%    90% 
##  594.0 3229.6 3591.4 3852.2 4154.2 4311.5 4570.4 4750.0 5055.0 5744.5 
##   100% 
## 7114.0</code></pre>
<pre class="r"><code># frequency of tokens; get the top ones 
freq &lt;- hongloumeng %&gt;%
  group_by(token) %&gt;%
  summarize(count = n()) %&gt;%
  top_n(n = 100, count) %&gt;%
  arrange(desc(count))

# set up the ususal format for text analysis
honglou2 &lt;- hongloumeng %&gt;%
    mutate(sid=1, tid=1, lemma=NA, upos=NA, pos=NA, cid=NA, word=token)

# assign first 80 chapters to Cao and the last 40 to unknown.
chapters &lt;- honglou2 %&gt;%
    group_by(id) %&gt;%
    summarise(chapter=mean(id)) %&gt;%
    mutate(author=ifelse(chapter&lt;81, &quot;cao&quot;, &quot;unknown&quot;))

# use cleanNLP&#39;s get_tfidf to get the term frequency matrix
# tfidf &lt;- honglou2 %&gt;%
#      get_tfidf(type = &quot;tfidf&quot;, tf_weight = &quot;dnorm&quot;, token_var=&quot;word&quot;)


#honglou_pca &lt;- tidy_pca(tfidf$tfidf, chapters)

#ggplot(honglou_pca, aes(PC1, PC2)) +
#  geom_point(aes(color=chapter))


#mat.honglou &lt;- get_tfidf(honglou2, min_df = 0.1, max_df = .9, type = &quot;tf&quot;,
#                 tf_weight = &quot;raw&quot;, doc_var = &quot;id&quot;, token_var=&quot;word&quot;)

# a second specification for the term freqency matrix.
mat.honglou &lt;- honglou2 %&gt;%
    get_tfidf(type = &quot;tf&quot;, tf_weight = &quot;dnorm&quot;, token_var=&quot;word&quot;)

## tf2 &lt;-  honglou2 %&gt;%
##     get_tfidf(min_df = 0, max_df = 1, type = &quot;tf&quot;,
##                  tf_weight = &quot;raw&quot;,  token_var=&quot;word&quot;)

# random assign traing and testing samples
set.seed(1)
chapters &lt;- chapters %&gt;%
    mutate(training = as.logical(rbinom(length(chapter), 1, .5))) %&gt;%
    mutate(y=as.numeric(author==&quot;cao&quot;))

chapters.train &lt;- chapters %&gt;% filter(training==1)
model &lt;- cv.glmnet(mat.honglou$tf[chapters$training,], chapters.train$y,
                   family = &quot;binomial&quot;, alpha=.9)

chapters$pred &lt;- predict(model, newx=mat.honglou$tf, type=&#39;response&#39;, s=model$lambda.1se)

## ggplot(chapters, aes(chapter, pred)) +
##   geom_boxplot(aes(fill = author))

ggplot(chapters, aes(x = chapter, y = pred, color = training))  +geom_point() +geom_vline(xintercept=80) +geom_vline(xintercept=108) + geom_text(aes(chapter-3,pred, label = chapter), data = chapters %&gt;% filter((pred&lt;.85 &amp; chapter&lt;81 &amp; training==0) | (pred &gt; .5 &amp; chapter&gt;80 &amp; training==0)))</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk2-1.png" width="672" /></p>
<p>在图中，我画了两条竖线，80和108回。有些人认为曹的原作只有108回。</p>
<p>两种不同颜色代表training or test。 我们看到如果说前80回作者是“Cao”，那么后40回作者不大可能是“Cao”。 甚至前80回中有几回也与其他章用词不太一致，例如10, 11, 60, 64, 67。 特别是67, 不太可能作者是“Cao”。 在后40回中， 84,98,119 则与前80回风格较为一致。</p>
</div>
<div class="section level1">
<h1>第二种模型</h1>
<p>第二种模型是某种程度上的改善。 在第一种模型中，我们把前80回标为“Cao”，后40回为“unknown”。我们没有引入不确定性。 这里，我们假设后40回有百分之二十可能作者是“Cao”。这样我们引入不确定性。 我们还是用同一种统计模型elastic net。</p>
<pre class="r"><code># here we introduce uncertainty: suppose the last 40 chapters have 20% chance been written by Cao.  

set.seed(1)
chapters.matrix &lt;- matrix(NA,120,100)
training.matrix &lt;- matrix(NA, 120, 100)
for (i in (1:100)){

chapters2 &lt;- honglou2 %&gt;%
    group_by(id) %&gt;%
    summarise(chapter=mean(id)) %&gt;%
    mutate(author=ifelse(chapter&lt;81, &quot;cao&quot;, ifelse(rbinom(40,1,.2),&quot;cao&quot;,&quot;unknown&quot;))) %&gt;%
    mutate(training = as.logical(rbinom(length(chapter), 1, .5))) %&gt;%
    mutate(y=as.numeric(author==&quot;cao&quot;))

chapters2.train &lt;- chapters2 %&gt;% filter(training==1)
model2 &lt;- cv.glmnet(mat.honglou$tf[chapters2$training,], chapters2.train$y,
                   family = &quot;binomial&quot;, alpha=.9)

chapters2$pred &lt;- predict(model2, newx=mat.honglou$tf, type=&#39;response&#39;, s=model2$lambda.1se)

chapters.matrix[, i] &lt;- chapters2$pred
training.matrix[,i] &lt;- chapters2$training

}
## ggplot(chapters, aes(chapter, pred)) +
##   geom_boxplot(aes(fill = author))

wide &lt;- bind_cols(data.frame(chapter=1:120), as.data.frame(chapters.matrix))
wide2 &lt;- bind_cols(data.frame(chapter=1:120), as.data.frame(training.matrix))
long &lt;- wide %&gt;% gather(&quot;sim&quot;, &quot;yhat&quot;,2:101)
long &lt;- long %&gt;% mutate(sim = as.integer(sub(&quot;V&quot;, &quot;&quot;, sim)))
long2 &lt;- wide2 %&gt;% gather(&quot;sim&quot;, &quot;training&quot;,2:101)
long2 &lt;- long2 %&gt;% mutate(sim = as.integer(sub(&quot;V&quot;, &quot;&quot;, sim)))
long3 &lt;- bind_cols(long, long2)[,c(1:3, 6)]

merged &lt;- long3 %&gt;% left_join(chapters2 %&gt;% select(-training), by = &quot;chapter&quot;)

#ggplot(merged, aes(y = yhat, x = chapter, group = chapter)) + geom_boxplot()
ggplot(merged, aes(y = yhat, x = chapter, group = chapter)) +
  geom_boxplot() + facet_grid(training ~ .)</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk3-1.png" width="672" /></p>
<p>在这里我画的是重复100次取样后的box plot， 上面是testing, 下面是training。 第二种模型和第一种结论类似，只是引入不确定性。</p>
</div>
<div class="section level1">
<h1>第三种模型</h1>
<p>前两次的模型有个显然的假设，就是我们假设前80回是同一个作者，然后我们来判断是否后40回是否属于同一作者。也就是说这里我们只能根据前80回为同一作者来推断后40回的作者。 如果前80回不是一个作者那么我们的推断就不成立。</p>
<p>我想到也许应该用一种更好的模型， 就是每一章都有一定的概率属于“Cao”。 然后我们来估计这些概率。我发现这属于“topic models”。</p>
<p>“topic models” 是用来研究文本中有几个主题的，但是我们没有理由不能把它用于研究作者。因为归根结底主题就是词汇组合，而作者也可视为词汇组合。</p>
<p>现在我们唯一的假设是有两个作者， 严格说，是有两个“主题”，或者说，这120回可以分为两组，至于两组为什么不同，我姑且认为是作者不同，因为我们是从用词频率分析的。 也可以假设有三个作者来分析，等等，分析方法都一样。</p>
<p>这里我们用LDA(Linear Dirichlet Allocation) 分析每一章属于“Cao” 的概率。</p>
<pre class="r"><code>set.seed(1)

mat.honglou2 &lt;- honglou2 %&gt;%
    get_tfidf(type = &quot;tf&quot;, tf_weight = &quot;raw&quot;, token_var=&quot;word&quot;,
    min_df=.05, max_df=.95)
colnames(mat.honglou2$tf) &lt;- mat.honglou2$vocab
lda2 &lt;- LDA(mat.honglou2$tf, k = 2, method=&quot;Gibbs&quot;, control = list(seed = 1, burnin = 1000, thin = 100, iter = 1000))

plot(topics(lda2))</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk4-1.png" width="672" /></p>
<pre class="r"><code>lda_topics &lt;- as.data.frame(posterior(lda2)$topics)
names(lda_topics) &lt;- c(&#39;prob1&#39;, &#39;prob2&#39;)

newdata &lt;- bind_cols(chapters, lda_topics)
topics &lt;- posterior(lda2)[[&quot;topics&quot;]]
#heatmap(topics, scale = &quot;none&quot;)

ggplot(newdata, aes(y = prob1, x = chapter, group = chapter)) +
 geom_point() +geom_vline(xintercept=80) +geom_vline(xintercept=108) + geom_text(aes(chapter-3,prob1, label = chapter), data = newdata %&gt;% filter((prob1&lt;.5 &amp; chapter&lt;81 ) | (prob1 &gt; .5 &amp; chapter&gt;80 )))</code></pre>
<p><img src="/post/2017-09-19-red-chamber_files/figure-html/chunk4-2.png" width="672" /></p>
<p>我们看到前80回有19回是作者1写的概率低于50%。 尤其第67回， 只有25%的概率是作者1写的。 在后40回中， 没有一回概率大于50%！ 作者1应该是曹雪芹。这里我们不能分析出作者是谁，只是说后40回的作者与前80回大多数章节的作者不同。</p>
<p>下面我们看看作者1,2最常用的前50个词， 严格说是从LDA模型中的posterior得到的主题1,2最常出现的50个词。</p>
<pre class="r"><code>terms1 &lt;- posterior(lda2)$terms[1,]
terms2 &lt;- posterior(lda2)$terms[2,]
head(sort(terms1, decreasing=TRUE), 50)</code></pre>
<pre><code>##          吃        贾母          至        众人          方          贾 
## 0.010980743 0.006512404 0.005828384 0.005401982 0.005401982 0.005330915 
##          凤          珍        一面        黛玉          字          酒 
## 0.004593594 0.004167192 0.004007291 0.003971758 0.003794090 0.003660840 
##          此        两个          花        如此          茶          皆 
## 0.003625306 0.003536472 0.003367688 0.003341038 0.003296621 0.003190021 
##        大家        他们          竟          亦        尤氏        晴雯 
## 0.003074537 0.003047887 0.003039003 0.003030120 0.002994586 0.002914636 
##          向          使          日          未          如        湘云 
## 0.002896869 0.002887986 0.002852452 0.002834686 0.002754735 0.002754735 
##          比        不过          老        听说          忽          三 
## 0.002674785 0.002621485 0.002612601 0.002577068 0.002568184 0.002541534 
##        薛蟠          其          或          香          所          毕 
## 0.002523767 0.002514884 0.002506001 0.002497117 0.002488234 0.002488234 
##          生          让          并        只见          后        原来 
## 0.002479351 0.002470467 0.002443817 0.002399400 0.002319450 0.002310566 
##          岂          而 
## 0.002301683 0.002301683</code></pre>
<pre class="r"><code>head(sort(terms2, decreasing=TRUE), 50)</code></pre>
<pre><code>##        袭人        凤姐        姑娘      王夫人        宝钗          给 
## 0.009255081 0.009173040 0.008836671 0.008730017 0.008286994 0.007753725 
##      老太太        贾母        丫头        太太        贾琏        你们 
## 0.007745521 0.007245069 0.006744617 0.006695392 0.006252369 0.006194940 
##        奶奶        贾政          死        平儿        老爷        听见 
## 0.006030857 0.005792938 0.005472976 0.005341710 0.005095586 0.005054566 
##        黛玉        告诉        东西        他们        回来          爷 
## 0.005038157 0.004865871 0.004562318 0.004307990 0.004307990 0.004225948 
##        进来          病          哭          做      薛姨妈        只见 
## 0.004193132 0.003963416 0.003897783 0.003782925 0.003717292 0.003586026 
##          瞧        紫鹃        鸳鸯        过来        这个        这样 
## 0.003577822 0.003405535 0.003405535 0.003356310 0.003339902 0.003274269 
##          找        咱们        心里          闹        这些        一面 
## 0.003266065 0.003159411 0.003118391 0.003077370 0.003044553 0.003028145 
##          跟          却        两个          林        这么        姐姐 
## 0.003019941 0.002987124 0.002831246 0.002782021 0.002749205 0.002699980 
##        所以        出去 
## 0.002699980 0.002683572</code></pre>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-106731399-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
  </body>
</html>

